{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "aY_B8XU_VVZd",
        "outputId": "02a23e9f-1b14-4004-9e52-37872581b3d4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Suppose we have a dataset with a feature representing ages of people ranging from 20 to 60. We want to scale these\\n  ages to a range between 0 and 1 using Min-Max scaling.\\n\\nOriginal ages:\\n\\nPerson 1: 20\\nPerson 2: 30\\nPerson 3: 40\\nPerson 4: 50\\nPerson 5: 60\\n'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#1\n",
        "\"\"\"\n",
        "Min-Max scaling, also known as normalization, is a technique used in data preprocessing to scale numeric\n",
        "features to a specific range. It rescales the data to have values between 0 and 1 (or any other specified range).\n",
        "This is achieved by subtracting the minimum value of the feature and then dividing by the range of the feature\n",
        "\"\"\"\n",
        "#example\n",
        "\"\"\"Suppose we have a dataset with a feature representing ages of people ranging from 20 to 60. We want to scale these\n",
        "  ages to a range between 0 and 1 using Min-Max scaling.\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "E6x2qoZAbmw2",
        "outputId": "afffe8df-e3b1-4321-99ea-99063ae95644"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'sklearn_'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-d4657575c1ef>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn_'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "#2\n",
        "\"\"\"\n",
        "\n",
        "The Unit Vector technique, also known as unit normalization or vector normalization, is a feature scaling method used to scale the values of a feature\n",
        " It involves dividing each feature vector by its magnitude, resulting in a vector with a length of 1.\n",
        "\n",
        "\n",
        "Unit vector normalization differs from Min-Max scaling in that it does not necessarily scale the features to a specific range like [0, 1]. Instead, it\n",
        "scales the features such that their magnitudes become 1, preserving the direction of the original vectors.\n",
        "\n",
        "\"\"\"\n",
        "#example\n",
        "import seaborn as sns\n",
        "from sklearn_.preprocessing import normalize\n",
        "df=sns.load_.dataset('iris')\n",
        "n=normalize(df[['sepal_length','sepal_width']])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "EGUsR-_ooF0n",
        "outputId": "e28f620b-d048-4938-c6b6-924cfafe9123"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n\\n'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#3\n",
        "\"\"\"\n",
        "\n",
        "Unsupervised learning involves training algorithms on datasets without labeled output. The system tries\n",
        "to learn the patterns and structure within the data without explicit guidance\n",
        "\n",
        "example:clustring,dimension reduction,Anomaly detection\n",
        "\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZjAcIOJHpqR"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "1WYv_FaIwffd",
        "outputId": "ec02e782-4bc4-4390-bb58-650ac29793c9"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nAI refers to the development of computer systems that can perform tasks that typically require human intelligence.\\nIt encompasses a broad range of techniques andapplications.\\n\\n ML is a subset of AI that focuses on developing algorithms and models that allow computers to learn patterns and make\\ndecisions based on data. ML systems improve their performance over time without being explicitly programmed.\\n\\n DL is a specialized form of machine learning that involves neural networks with many layers (deep neural networks).\\nThese deep architectures enable the automatic learning of features from data.\\n\\nData science is a multidisciplinary field that involves extracting insights and knowledge from data through various methods,\\nincluding statistical analysis, machine learning, and data visualization.\\n'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#4\n",
        "\"\"\"\n",
        "AI refers to the development of computer systems that can perform tasks that typically require human intelligence.\n",
        "It encompasses a broad range of techniques andapplications.\n",
        "\n",
        " ML is a subset of AI that focuses on developing algorithms and models that allow computers to learn patterns and make\n",
        "decisions based on data. ML systems improve their performance over time without being explicitly programmed.\n",
        "\n",
        " DL is a specialized form of machine learning that involves neural networks with many layers (deep neural networks).\n",
        "These deep architectures enable the automatic learning of features from data.\n",
        "\n",
        "Data science is a multidisciplinary field that involves extracting insights and knowledge from data through various methods,\n",
        "including statistical analysis, machine learning, and data visualization.\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "yM3iOYPlxAmA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-gSsw_ist4bj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "45ce9bad-b19f-4ea0-af83-a11aed221118"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\nMin-Max scaling is a preprocessing technique used to scale features to a specific range, typically between 0 and 1.\\nIt's particularly useful when dealing with features that have different scales and may be measured in different units. Here's how we can\\nuse Min-Max scaling to preprocess the data for your food delivery service recommendation system:\\n\\nBy using Min-Max scaling, you ensure that all features are on the same scale, which can prevent features with larger ranges from dominating\\nthe model training process. This preprocessing step can lead to better performance and more stable model training for your food delivery service recommendation system.\\n\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "#5\n",
        "\"\"\"\n",
        "\n",
        "Min-Max scaling is a preprocessing technique used to scale features to a specific range, typically between 0 and 1.\n",
        "It's particularly useful when dealing with features that have different scales and may be measured in different units. Here's how we can\n",
        "use Min-Max scaling to preprocess the data for your food delivery service recommendation system:\n",
        "\n",
        "By using Min-Max scaling, you ensure that all features are on the same scale, which can prevent features with larger ranges from dominating\n",
        "the model training process. This preprocessing step can lead to better performance and more stable model training for your food delivery service recommendation system.\n",
        "\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6DlacnW5xu49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "842d18ea-30dc-492d-a20f-dc38a67bb6aa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nPrincipal Component Analysis (PCA) is a technique used for dimensionality reduction in datasets. In the context of building a model to predict stock prices using a dataset\\ncontaining numerous features such as company financial data and market trends.\\nBy applying PCA, you effectively reduce the number of features in the dataset while retaining most of the variance present in the data. This can help in reducing overfitting,\\nspeeding up training, and improving the generalization performance of your stock price prediction model.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "#6\n",
        "\"\"\"\n",
        "Principal Component Analysis (PCA) is a technique used for dimensionality reduction in datasets. In the context of building a model to predict stock prices using a dataset\n",
        "containing numerous features such as company financial data and market trends.\n",
        "By applying PCA, you effectively reduce the number of features in the dataset while retaining most of the variance present in the data. This can help in reducing overfitting,\n",
        "speeding up training, and improving the generalization performance of your stock price prediction model.\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Y7hP7BHC2wjJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0553583e-77d4-4fd5-d1e9-5cf89ae2208a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        ],\n",
              "       [0.05263158],\n",
              "       [0.47368421],\n",
              "       [1.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "#7\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "df=np.array([1,2,10,20]).reshape(-1,1)\n",
        "m=MinMaxScaler()\n",
        "m.fit_transform(df)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "8g0P9N5fGou1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "636fffc2-fd00-4643-d9fe-265529673c9b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-8.78226558e-01,  1.56554793e+00, -7.01592742e-17],\n",
              "       [-1.39221138e+01, -8.54576977e-01,  5.55111512e-15],\n",
              "       [ 1.48003404e+01, -7.10970958e-01,  4.66293670e-15]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "#8\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "\n",
        "data = np.array([\n",
        "    [170, 65, 30, 0, 120],\n",
        "    [165, 60, 35, 1, 130],\n",
        "    [180, 70, 25, 1, 110],\n",
        "\n",
        "])\n",
        "\n",
        "pca = PCA()\n",
        "pca.fit(data)\n",
        "pca.transform(data)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-f-vOvqMmXp"
      },
      "outputs": [],
      "source": [
        "np.array()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HEFnm_shXF9u",
        "outputId": "71890079-3114-41dc-b30c-09dfefb77558"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    S\n",
              "Name: embarked, dtype: object"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hv62ESsKXPhu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3pd3AMoXU9l"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Tga6sDLcsMup"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}